import functools
from typing import Dict, List, Optional, Tuple
import pickle
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from pyhealth.datasets import BaseSignalDataset
from pyhealth.models import BaseModel, ResBlock2D, EmbeddingModel


class VAE(BaseModel):
    """VAE model for images or time-series data.

    Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes."

    Supports both image generation/reconstruction and time-series modeling.
    Images mode take 128x128 or 64x64 or 32x32 images.

    Args:
        dataset: the dataset to train the model. It is used to query certain
            information such as the set of all tokens.
        feature_keys:  list of keys in samples to use as features,
            e.g. ["conditions", "procedures"].
        label_key: key in samples to use as label (e.g., "drugs").
        mode: one of "binary", "multiclass", or "multilabel".
        input_type: 'image' for CNN-based VAE on images, 'timeseries' for RNN-based on sequences. Default 'image'.
        input_channel: number of input channels (for images). Required if input_type='image'.
        input_size: input image size (for images, e.g. 128). Required if input_type='image'.
        hidden_dim: the latent dimension. Default is 128.
        conditional_feature_keys: list of feature keys to use as conditions for generation (optional).
        **kwargs: other parameters.

    Examples:
    """

    def __init__(
        self,
        dataset,
        feature_keys: List[str],
        label_key: str,
        mode: str,
        input_type: str = "image",
        input_channel: Optional[int] = None,
        input_size: Optional[int] = None,
        hidden_dim: int = 128,
        conditional_feature_keys: Optional[List[str]] = None,
        **kwargs,
    ):
        super(VAE, self).__init__(
            dataset=dataset,
            feature_keys=feature_keys,
            label_key=label_key,
            mode=mode,
        )
        self.input_type = input_type
        self.hidden_dim = hidden_dim
        self.conditional_feature_keys = conditional_feature_keys

        if input_type == "image":
            assert input_channel is not None and input_size is not None, "For image mode, input_channel and input_size must be provided"
            if conditional_feature_keys:
                self.embedding_model = EmbeddingModel(dataset, embedding_dim=hidden_dim)

            # encoder part
            if input_size == 128:
                self.encoder1 = nn.Sequential(
                    ResBlock2D(input_channel, 16, 2, True, True),
                    ResBlock2D(16, 64, 2, True, True),
                    ResBlock2D(64, 256, 2, True, True),
                )
                self.mu = nn.Linear(256 * 2 * 2, self.hidden_dim) # for mu
                self.log_std2 = nn.Linear(256 * 2 * 2, self.hidden_dim) # for log (sigma^2)

                self.decoder1 = nn.Sequential(
                    nn.ConvTranspose2d(self.hidden_dim, 256, kernel_size=5, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(32, input_channel, kernel_size=6, stride=2),
                    nn.Sigmoid(),
                )

            elif input_size == 64:
                self.encoder1 = nn.Sequential(
                    ResBlock2D(input_channel, 16, 2, True, True),
                    ResBlock2D(16, 64, 2, True, True),
                    ResBlock2D(64, 256, 2, True, True),
                )
                self.mu = nn.Linear(256, self.hidden_dim) # for mu
                self.log_std2 = nn.Linear(256, self.hidden_dim) # for log (sigma^2)

                self.decoder1 = nn.Sequential(
                    nn.ConvTranspose2d(self.hidden_dim, 128, kernel_size=5, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(32, input_channel, kernel_size=6, stride=2),
                    nn.Sigmoid(),
                )

            elif input_size == 32:
                self.encoder1 = nn.Sequential(
                    ResBlock2D(input_channel, 16, 2, True, True),
                    ResBlock2D(16, 64, 2, True, True),
                    # ResBlock2D(64, 256, 2, True, True),
                )
                self.mu = nn.Linear(64 * 2 * 2, self.hidden_dim) # for mu
                self.log_std2 = nn.Linear(64 * 2 * 2, self.hidden_dim) # for log (sigma^2)

                self.decoder1 = nn.Sequential(
                    nn.ConvTranspose2d(self.hidden_dim, 64, kernel_size=5, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),
                    nn.ReLU(),
                    nn.ConvTranspose2d(32, input_channel, kernel_size=6, stride=2),
                    nn.Sigmoid(),
                )

        elif input_type == "timeseries":
            self.embedding_model = EmbeddingModel(dataset, embedding_dim=hidden_dim)
            self.encoder_rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)
            self.mu = nn.Linear(hidden_dim, hidden_dim)
            self.log_std2 = nn.Linear(hidden_dim, hidden_dim)
            self.decoder_linear = nn.Linear(hidden_dim, hidden_dim)

        else:
            raise ValueError("input_type must be 'image' or 'timeseries'")
            
    def encoder(self, x) -> Tuple[torch.Tensor, torch.Tensor]:
        if self.input_type == "image":
            h = self.encoder1(x)
            batch_size = h.shape[0]
            h = h.view(batch_size, -1)
            mu = self.mu(h)
            std = torch.sqrt(torch.exp(self.log_std2(h)))
        elif self.input_type == "timeseries":
            # x is dict of embedded features
            # assume x is already embedded dict
            # for simplicity, concatenate all embedded features
            embedded_list = []
            for key, emb in x.items():
                if emb.dim() == 3:  # (batch, seq, emb)
                    # use RNN to aggregate sequence
                    _, h = self.encoder_rnn(emb)
                    h = h.squeeze(0)  # (batch, hidden)
                else:
                    h = emb  # (batch, emb)
                embedded_list.append(h)
            h = torch.cat(embedded_list, dim=-1) if len(embedded_list) > 1 else embedded_list[0]
            # if multiple, perhaps mean or something, but for now cat and linear
            if h.shape[-1] != self.hidden_dim:
                h = nn.Linear(h.shape[-1], self.hidden_dim)(h)
            mu = self.mu(h)
            std = torch.sqrt(torch.exp(self.log_std2(h)))
        return mu, std
    
    def sampling(self, mu, std) -> torch.Tensor: # reparameterization trick
        eps = torch.randn_like(std)
        return mu + eps * std

    def decoder(self, z) -> torch.Tensor:
        if self.input_type == "image":
            x_hat = self.decoder1(z)
        elif self.input_type == "timeseries":
            x_hat = self.decoder_linear(z)
        return x_hat
    
    @staticmethod
    def loss_function(y, x, mu, std): 
        ERR = F.binary_cross_entropy(y, x, reduction='sum')
        KLD = -0.5 * torch.sum(1 + torch.log(std**2) - mu**2 - std**2)
        return ERR + KLD
    
    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:

        if self.input_type == "image":
            # concat the info within one batch (batch, channel, height, width)
            # if the input is a list of numpy array, we need to convert it to tensor
            if isinstance(kwargs[self.feature_keys[0]][0], np.ndarray):
                x = torch.tensor(
                    np.array(kwargs[self.feature_keys[0]]).astype("float16"), device=self.device
                ).float()
            else:
                x = torch.stack(kwargs[self.feature_keys[0]], dim=0).to(self.device)

            mu, std = self.encoder(x)
            z = self.sampling(mu, std)

            # add conditional embeddings if provided
            if self.conditional_feature_keys:
                cond_emb = self.embedding_model({k: kwargs[k] for k in self.conditional_feature_keys})
                # assume cond_emb is dict, concatenate all
                cond_list = [emb.mean(dim=1) if emb.dim() == 3 else emb for emb in cond_emb.values()]
                cond_vec = torch.cat(cond_list, dim=-1) if len(cond_list) > 1 else cond_list[0]
                if cond_vec.shape[-1] != self.hidden_dim:
                    cond_vec = nn.Linear(cond_vec.shape[-1], self.hidden_dim)(cond_vec)
                z = z + cond_vec  # or concatenate, but for simplicity add

            z = z.unsqueeze(2).unsqueeze(3)
            x_rec = self.decoder(z)

        elif self.input_type == "timeseries":
            # embed all feature_keys
            embedded = self.embedding_model({k: kwargs[k] for k in self.feature_keys})
            mu, std = self.encoder(embedded)
            z = self.sampling(mu, std)
            x_rec = self.decoder(z)
            # for timeseries, x is the embedded target or something, but for simplicity, use embedded as x
            x = torch.cat(list(embedded.values()), dim=-1) if len(embedded) > 1 else list(embedded.values())[0]

        loss = self.loss_function(x_rec, x, mu, std)
        results = {
            "loss": loss,
            "y_prob": x_rec,
            "y_true": x,
        }
        return results


if __name__ == "__main__":
    from pyhealth.datasets import SampleSignalDataset, get_dataloader
    from pyhealth.datasets import COVID19CXRDataset
    from torchvision import transforms
    
    root = "/srv/local/data/COVID-19_Radiography_Dataset"
    base_dataset = COVID19CXRDataset(root, dev=True, refresh_cache=False)
    
    sample_dataset = base_dataset.set_task()

    # the transformation automatically normalize the pixel intensity into [0, 1]
    transform = transforms.Compose([
        transforms.Lambda(lambda x: x if x.shape[0] == 3 else x.repeat(3, 1, 1)), # only use the first channel
        transforms.Resize((128, 128)),
    ])

    def encode(sample):
        sample["path"] = transform(sample["path"])
        return sample

    sample_dataset.set_transform(encode)

    # data loader
    from pyhealth.datasets import get_dataloader

    train_loader = get_dataloader(sample_dataset, batch_size=2, shuffle=True)

    # model
    model = VAE(
        dataset=sample_dataset,
        feature_keys=["path"],
        label_key="path",
        mode="regression",
        input_type="image",
        input_channel=3,
        input_size=128,
        hidden_dim=256,
    ).to("cuda")

    # data batch
    data_batch = next(iter(train_loader))

    # try the model
    ret = model(**data_batch)
    print(ret)

    # try loss backward
    ret["loss"].backward()