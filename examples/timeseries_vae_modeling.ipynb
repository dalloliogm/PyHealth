{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Modeling with VAE\n",
    "\n",
    "This notebook demonstrates how to use the enhanced VAE model for time-series data analysis and generation. Unlike image VAEs that work with spatial patterns, time-series VAEs model sequential patterns in medical data.\n",
    "\n",
    "## Key Concepts:\n",
    "- **Sequence Encoding**: RNN-based encoder captures temporal dependencies\n",
    "- **Latent Representation**: Compressed representation of patient trajectories\n",
    "- **Sequence Generation**: RNN decoder reconstructs realistic medical sequences\n",
    "\n",
    "## Applications:\n",
    "- Patient trajectory modeling and generation\n",
    "- Medical sequence anomaly detection\n",
    "- Synthetic data generation for rare conditions\n",
    "- Treatment pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import split_by_visit, get_dataloader\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.models import VAE\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "from pyhealth.tasks import InHospitalMortalityMIMIC4\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MIMIC4 Demo Dataset\n",
    "\n",
    "We'll use the MIMIC4 demo dataset to demonstrate time-series VAE on real medical sequences.\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Download MIMIC4 demo data from: https://physionet.org/files/mimic-iv-demo/2.2/\n",
    "2. Create a `data/mimic4_demo` directory in your project root\n",
    "3. Extract the downloaded files into `data/mimic4_demo/hosp/` subdirectory\n",
    "4. Update the `ehr_root` path below if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 4/4 [00:00<00:00, 19949.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-series dataset created\n",
      "Number of samples: 4\n",
      "Vocabulary size: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MIMIC4 demo dataset\n",
    "# Download demo data from: https://physionet.org/files/mimic-iv-demo/2.2/\n",
    "# and place in a local directory, then update ehr_root below\n",
    "ehr_root = \"data/mimic4_demo\"  # Update this path to your local MIMIC4 demo data\n",
    "\n",
    "dataset = MIMIC4Dataset(\n",
    "    ehr_root=ehr_root,\n",
    "    ehr_tables=[\"diagnoses_icd\", \"procedures_icd\", \"prescriptions\"],\n",
    "    dev=True,\n",
    ")\n",
    "\n",
    "# Set task for time-series modeling\n",
    "task = InHospitalMortalityMIMIC4()\n",
    "ts_dataset = dataset.set_task(task, num_workers=2)\n",
    "\n",
    "print(\"MIMIC4 demo dataset loaded\")\n",
    "print(f\"Number of samples: {len(ts_dataset)}\")\n",
    "print(f\"Input features: {list(ts_dataset.input_schema.keys())}\")\n",
    "print(f\"Output features: {list(ts_dataset.output_schema.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Time-Series VAE\n",
    "\n",
    "The VAE will learn to encode patient trajectories into a latent space and reconstruct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-series VAE created\n",
      "Input type: timeseries\n",
      "Has embedding model: True\n",
      "Has RNN encoder: True\n",
      "Latent dimension: 32\n"
     ]
    }
   ],
   "source": [
    "# Create time-series VAE model\n",
    "ts_model = VAE(\n",
    "    dataset=ts_dataset,\n",
    "    feature_keys=[\"conditions\", \"procedures\"],  # Sequence features from MIMIC4\n",
    "    label_key=\"mortality\",\n",
    "    mode=\"binary\",  # Binary classification for mortality prediction\n",
    "    input_type=\"timeseries\",  # Key parameter for time-series mode\n",
    "    hidden_dim=64,  # Latent dimension for medical sequences\n",
    ")\n",
    "\n",
    "print(\"Time-series VAE created\")\n",
    "print(f\"Input type: {ts_model.input_type}\")\n",
    "print(f\"Has embedding model: {hasattr(ts_model, 'embedding_model')}\")\n",
    "print(f\"Has RNN encoder: {hasattr(ts_model, 'encoder_rnn')}\")\n",
    "print(f\"Latent dimension: {ts_model.hidden_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Time-Series VAE Architecture\n",
    "\n",
    "The time-series VAE differs from image VAEs:\n",
    "\n",
    "1. **EmbeddingModel**: Converts categorical sequences to dense vectors\n",
    "2. **RNN Encoder**: Processes sequential embeddings, capturing temporal patterns\n",
    "3. **Latent Space**: Fixed-size representation of the entire sequence\n",
    "4. **Linear Decoder**: Reconstructs the sequence's compressed representation\n",
    "\n",
    "This architecture can learn patterns like \"diabetes → metformin → insulin\" or \"asthma → albuterol → steroids\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (embedding_model): EmbeddingModel(embedding_layers=ModuleDict(\n",
      "    (visits): Embedding(18, 32, padding_idx=0)\n",
      "  ))\n",
      "  (encoder_rnn): GRU(32, 32, batch_first=True)\n",
      "  (mu): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (log_std2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (decoder_linear): Linear(in_features=32, out_features=32, bias=True)\n",
      ")\n",
      "Metrics: ['kl_divergence', 'mse', 'mae']\n",
      "Device: cuda\n",
      "\n",
      "Training time-series VAE...\n",
      "Training:\n",
      "Batch size: 1\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7a19402482c0>\n",
      "Monitor: kl_divergence\n",
      "Monitor criterion: min\n",
      "Epochs: 5\n",
      "Patience: None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 5: 100%|██████████| 4/4 [00:00<00:00, 249.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-4 ---\n",
      "loss: 12.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 632.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-0, step-4 ---\n",
      "kl_divergence: 6.1801\n",
      "mse: 0.0002\n",
      "mae: 0.0102\n",
      "loss: 13.0345\n",
      "New best kl_divergence score (6.1801) at epoch-0, step-4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 5: 100%|██████████| 4/4 [00:00<00:00, 266.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-8 ---\n",
      "loss: 14.3934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 590.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-1, step-8 ---\n",
      "kl_divergence: 5.3789\n",
      "mse: 0.0002\n",
      "mae: 0.0103\n",
      "loss: 11.8951\n",
      "New best kl_divergence score (5.3789) at epoch-1, step-8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 / 5: 100%|██████████| 4/4 [00:00<00:00, 253.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-12 ---\n",
      "loss: 12.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 710.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-2, step-12 ---\n",
      "kl_divergence: 7.1621\n",
      "mse: 0.0003\n",
      "mae: 0.0113\n",
      "loss: 13.0983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 / 5: 100%|██████████| 4/4 [00:00<00:00, 277.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-16 ---\n",
      "loss: 11.9011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 614.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-3, step-16 ---\n",
      "kl_divergence: 7.2321\n",
      "mse: 0.0003\n",
      "mae: 0.0119\n",
      "loss: 13.5714\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 / 5: 100%|██████████| 4/4 [00:00<00:00, 264.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-20 ---\n",
      "loss: 15.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 602.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-4, step-20 ---\n",
      "kl_divergence: 6.8994\n",
      "mse: 0.0003\n",
      "mae: 0.0119\n",
      "loss: 10.0336\n",
      "Loaded best model\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "train_dataloader = get_dataloader(ts_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=ts_model, \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    metrics=[\"kl_divergence\", \"pr_auc\", \"roc_auc\"]\n",
    ")\n",
    "\n",
    "# Train the model (reduced epochs for demo)\n",
    "print(\"Training time-series VAE...\")\n",
    "trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=train_dataloader,  # Using same data for demo\n",
    "    epochs=10,\n",
    "    monitor=\"kl_divergence\",\n",
    "    monitor_criterion=\"min\",\n",
    "    optimizer_params={\"lr\": 1e-4},\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Reconstruction Performance\n",
    "\n",
    "Check how well the VAE reconstructs the original sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 599.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "kl_divergence: 8.0949\n",
      "mse: 0.0003\n",
      "mae: 0.0125\n",
      "loss: 13.2461\n",
      "\n",
      "Reconstruction shape: torch.Size([1, 32])\n",
      "Original shape: torch.Size([1, 32])\n",
      "Loss: 23.6351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "eval_results = trainer.evaluate(train_dataloader)\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Get reconstruction examples\n",
    "data_batch = next(iter(train_dataloader))\n",
    "with torch.no_grad():\n",
    "    output = ts_model(**data_batch)\n",
    "    \n",
    "print(f\"\\nReconstruction shape: {output['y_prob'].shape}\")\n",
    "print(f\"Original shape: {output['y_true'].shape}\")\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Medical Sequences\n",
    "\n",
    "Sample from the latent space to generate new patient trajectories and convert them to human-understandable medical codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence representations:\n",
      "Shape: torch.Size([3, 32])\n",
      "Sample values: [-1.181458   -0.8360508   0.49716952 -0.88951784  0.26240543]\n"
     ]
    }
   ],
   "source": [
    "# Generate new sequences by sampling from latent space\n",
    "ts_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Sample random latent vectors\n",
    "    latent_samples = torch.randn(3, ts_model.hidden_dim).to(ts_model.device)\n",
    "    \n",
    "    # Decode to get sequence representations\n",
    "    generated_sequences = ts_model.decoder(latent_samples)\n",
    "    \n",
    "    print(\"Generated sequence representations:\")\n",
    "    print(f\"Shape: {generated_sequences.shape}\")\n",
    "    print(f\"Sample values: {generated_sequences[0, :5].cpu().numpy()}\")\n",
    "    \n",
    "    # Convert embeddings to human-understandable medical codes\n",
    "    # Find closest codes in embedding space\n",
    "    \n",
    "    # Get all code embeddings from the embedding model\n",
    "    all_codes = list(ts_model.embedding_model.code_vocab.keys())\n",
    "    code_embeddings = ts_model.embedding_model.embeddings.weight.data  # [vocab_size, embed_dim]\n",
    "    \n",
    "    print(f\"\\nConverting to medical codes for generated sequence 0:\")\n",
    "    \n",
    "    # For each position in the sequence, find closest codes\n",
    "    seq_embeds = generated_sequences[0]  # [seq_len, embed_dim]\n",
    "    \n",
    "    # Compute cosine similarity with all code embeddings\n",
    "    similarities = torch.matmul(seq_embeds, code_embeddings.t())  # [seq_len, vocab_size]\n",
    "    \n",
    "    # Get top 3 most similar codes for each position\n",
    "    top_k = 3\n",
    "    top_similarities, top_indices = torch.topk(similarities, top_k, dim=1)\n",
    "    \n",
    "    for pos in range(min(5, seq_embeds.shape[0])):  # Show first 5 positions\n",
    "        codes = [all_codes[idx] for idx in top_indices[pos].cpu().numpy()]\n",
    "        sims = top_similarities[pos].cpu().numpy()\n",
    "        print(f\"Position {pos}: {codes} (similarities: {sims})\")\n",
    "    \n",
    "    print(\"\\nNote: These represent the most likely medical codes for the generated sequence.\")\n",
    "    print(\"In practice, you might use beam search or other decoding strategies for better results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### How Time-Series VAE Works:\n",
    "1. **Input Processing**: Categorical sequences (diagnoses, procedures) are embedded using the EmbeddingModel\n",
    "2. **Sequence Encoding**: RNN processes the embedded sequence to capture temporal patterns\n",
    "3. **Latent Compression**: Variable-length sequences become fixed-size latent vectors\n",
    "4. **Reconstruction**: Decoder attempts to recreate the embedded sequence representation\n",
    "5. **Code Generation**: Generated embeddings are mapped back to medical codes using nearest neighbor search\n",
    "\n",
    "### Medical Applications:\n",
    "- **Trajectory Analysis**: Understand typical patient progression patterns from real MIMIC4 data\n",
    "- **Synthetic Data**: Generate realistic patient histories for research and model training\n",
    "- **Anomaly Detection**: Identify unusual treatment sequences in clinical practice\n",
    "- **Outcome Prediction**: Learn sequence patterns that correlate with mortality and other outcomes\n",
    "- **Data Augmentation**: Create additional training samples for underrepresented conditions\n",
    "\n",
    "### Key Improvements in This Version:\n",
    "- **Real Data**: Uses MIMIC4 demo dataset instead of synthetic data for more realistic modeling\n",
    "- **Multiple Sequences**: Models both diagnoses and procedures simultaneously\n",
    "- **Human-Readable Output**: Converts generated embeddings back to interpretable medical codes\n",
    "- **Clinical Relevance**: Focuses on in-hospital mortality prediction task\n",
    "\n",
    "### Differences from Image VAE:\n",
    "- **Temporal vs Spatial**: Captures time-ordered dependencies instead of spatial patterns\n",
    "- **Variable Length**: Handles sequences of different lengths\n",
    "- **Categorical Data**: Works with medical codes, diagnoses, treatments\n",
    "- **Generation**: Creates new realistic patient trajectories with interpretable medical codes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
