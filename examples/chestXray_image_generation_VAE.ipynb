{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-Ray Image Generation using VAE\n",
    "\n",
    "This notebook illustrates how to use the VAE module to generate X-Ray images, including the new features for conditional generation and time-series support.\n",
    "\n",
    "We will take the COVID-19 CXR dataset as starting point. This dataset is freely available on Kaggle and contains images of Chest X-Rays from COVID-19 patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Data is available from Kaggle. If it is not already available locally, download it with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download command (uncomment to run)\n",
    "# !curl -L -o ~/Downloads/covid19-radiography-database.zip https://www.kaggle.com/api/v1/datasets/download/tawsifurrahman/covid19-radiography-database\n",
    "# !unzip ~/Downloads/covid19-radiography-database.zip -d ~/Downloads/COVID-19_Radiography_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data with PyHealth Datasets\n",
    "\n",
    "Use the COVID19CXRDataset to load this data. For custom datasets, see the `BaseImageDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PyHealth/pyhealth/trainer.py:12: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n",
      "/home/ubuntu/PyHealth/pyhealth/sampler/sage_sampler.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.datasets import split_by_visit, get_dataloader\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.datasets import COVID19CXRDataset\n",
    "from pyhealth.models import VAE\n",
    "from pyhealth.processors import ImageProcessor\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No config path provided, using default config\n",
      "Initializing covid19_cxr dataset from /home/ubuntu/Downloads/COVID-19_Radiography_Dataset (dev mode: False)\n",
      "Scanning table: covid19_cxr from /home/ubuntu/Downloads/COVID-19_Radiography_Dataset/covid19_cxr-metadata-pyhealth.csv\n",
      "Setting task COVID19CXRClassification for covid19_cxr base dataset...\n",
      "Generating samples with 1 worker(s)...\n",
      "Collecting global event dataframe...\n",
      "Collected dataframe with shape: (21165, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for COVID19CXRClassification with 1 worker: 100%|██████████| 21165/21165 [00:08<00:00, 2354.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label disease vocab: {'COVID': 0, 'Lung Opacity': 1, 'Normal': 2, 'Viral Pneumonia': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing samples: 100%|██████████| 21165/21165 [01:30<00:00, 233.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 21165 samples for task COVID19CXRClassification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SampleDataset' object has no attribute 'set_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     sample[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m] = transform(sample[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43msample_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_transform\u001b[49m(encode)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SampleDataset' object has no attribute 'set_transform'"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data\n",
    "root = \"/home/ubuntu/Downloads/COVID-19_Radiography_Dataset\"\n",
    "base_dataset = COVID19CXRDataset(root)\n",
    "\n",
    "# Step 2: Set task with custom image processing for VAE\n",
    "image_processor = ImageProcessor(image_size=128, mode=\"L\")  # Resize to 128x128 for VAE\n",
    "sample_dataset = base_dataset.set_task(input_processors={\"image\": image_processor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys: dict_keys(['image', 'disease'])\n",
      "Dataset sizes - Train: 12699, Val: 4233, Test: 4233\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = split_by_visit(\n",
    "    sample_dataset, [0.6, 0.2, 0.2]\n",
    ")\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Check data\n",
    "data = next(iter(train_dataloader))\n",
    "print(\"Data keys:\", data.keys())\n",
    "print(\"Image shape:\", data[\"image\"][0].shape)\n",
    "print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic VAE Training (Image Generation)\n",
    "\n",
    "Train a standard VAE for unconditional image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder1): Sequential(\n",
      "    (0): ResBlock2D(\n",
      "      (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ELU(alpha=1.0)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (downsampler): Sequential(\n",
      "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): ResBlock2D(\n",
      "      (conv1): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ELU(alpha=1.0)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (downsampler): Sequential(\n",
      "        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (2): ResBlock2D(\n",
      "      (conv1): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ELU(alpha=1.0)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (downsampler): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (mu): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (log_std2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (decoder1): Sequential(\n",
      "    (0): ConvTranspose2d(128, 256, kernel_size=(5, 5), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))\n",
      "    (7): ReLU()\n",
      "    (8): ConvTranspose2d(32, 3, kernel_size=(6, 6), stride=(2, 2))\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Metrics: ['kl_divergence', 'mse', 'mae']\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 256\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7664b01baab0>\n",
      "Monitor: kl_divergence\n",
      "Monitor criterion: min\n",
      "Epochs: 5\n",
      "Patience: None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 5:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m trainer = Trainer(model=model, device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     15\u001b[39m                  metrics=[\u001b[33m\"\u001b[39m\u001b[33mkl_divergence\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Train (reduce epochs for demo)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced for demo\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkl_divergence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor_criterion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyHealth/pyhealth/trainer.py:206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataloader, val_dataloader, test_dataloader, epochs, optimizer_class, optimizer_params, steps_per_epoch, evaluation_steps, weight_decay, max_grad_norm, monitor, monitor_criterion, load_best_model_at_last, patience)\u001b[39m\n\u001b[32m    204\u001b[39m     data = \u001b[38;5;28mnext\u001b[39m(data_iterator)\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m loss = output[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# backward\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyHealth/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyHealth/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyHealth/pyhealth/models/vae.py:187\u001b[39m, in \u001b[36mVAE.forward\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_type == \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         x = \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    189\u001b[39m         mu, std = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m    190\u001b[39m         z = \u001b[38;5;28mself\u001b[39m.sampling(mu, std)\n",
      "\u001b[31mKeyError\u001b[39m: 'path'"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = VAE(\n",
    "    dataset=sample_dataset,\n",
    "    feature_keys=[\"image\"],\n",
    "    label_key=\"image\",\n",
    "    mode=\"regression\",\n",
    "    input_type=\"image\",\n",
    "    input_channel=1,  # Grayscale images from COVID dataset\n",
    "    input_size=128,  # Resized for VAE\n",
    "    hidden_dim=128,\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(model=model, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                 metrics=[\"kl_divergence\", \"mse\", \"mae\"])\n",
    "\n",
    "# Train (reduce epochs for demo)\n",
    "trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=5,  # Reduced for demo\n",
    "    monitor=\"kl_divergence\",\n",
    "    monitor_criterion=\"min\",\n",
    "    optimizer_params={\"lr\": 1e-3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"Evaluation results:\")\n",
    "eval_results = trainer.evaluate(test_dataloader)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Real vs Reconstructed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real and reconstructed images\n",
    "X, X_rec, _ = trainer.inference(test_dataloader)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.imshow(X[0].reshape(128, 128), cmap=\"gray\")\n",
    "ax1.set_title(\"Real Chest X-Ray\")\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(X_rec[0].reshape(128, 128), cmap=\"gray\")\n",
    "ax2.set_title(\"Reconstructed by VAE\")\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chestxray_vae_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Random Image Generation\n",
    "\n",
    "Generate new images by sampling from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic images\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample from latent space\n",
    "    z = torch.randn(1, model.hidden_dim).to(model.device)\n",
    "    \n",
    "    # Reshape for decoder (add spatial dims)\n",
    "    z = z.unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "    # Generate image\n",
    "    generated = model.decoder(z).detach().cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(generated[0].reshape(128, 128), cmap=\"gray\")\n",
    "    plt.title(\"Generated Chest X-Ray\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"chestxray_vae_synthetic.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature: Conditional VAE\n",
    "\n",
    "Generate images conditioned on additional features (e.g., diagnosis codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with conditional features\n",
    "samples_with_conditions = [\n",
    "    {\n",
    "        \"patient_id\": \"patient-0\",\n",
    "        \"visit_id\": \"visit-0\",\n",
    "        \"path\": torch.rand(3, 128, 128),  # Dummy image\n",
    "        \"conditions\": [\"COVID-19\", \"pneumonia\"],\n",
    "        \"label\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"patient_id\": \"patient-1\",\n",
    "        \"visit_id\": \"visit-1\",\n",
    "        \"path\": torch.rand(3, 128, 128),\n",
    "        \"conditions\": [\"normal\"],\n",
    "        \"label\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "from pyhealth.datasets import SampleDataset\n",
    "\n",
    "cond_dataset = SampleDataset(\n",
    "    samples=samples_with_conditions,\n",
    "    input_schema={\"path\": \"tensor\", \"conditions\": \"sequence\"},\n",
    "    output_schema={\"label\": \"binary\"},\n",
    "    dataset_name=\"conditional_demo\",\n",
    ")\n",
    "\n",
    "# Conditional VAE model\n",
    "cond_model = VAE(\n",
    "    dataset=cond_dataset,\n",
    "    feature_keys=[\"path\"],\n",
    "    label_key=\"label\",\n",
    "    mode=\"binary\",\n",
    "    input_type=\"image\",\n",
    "    input_channel=3,\n",
    "    input_size=128,\n",
    "    hidden_dim=64,\n",
    "    conditional_feature_keys=[\"conditions\"],  # New parameter\n",
    ")\n",
    "\n",
    "print(\"Conditional VAE created with embedding model for conditions\")\n",
    "print(f\"Has embedding model: {hasattr(cond_model, 'embedding_model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature: Time-Series VAE\n",
    "\n",
    "Use VAE for time-series data reconstruction and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-series dataset\n",
    "ts_samples = [\n",
    "    {\n",
    "        \"patient_id\": \"patient-0\",\n",
    "        \"visit_id\": \"visit-0\",\n",
    "        \"visits\": [\"diagnosis1\", \"diagnosis2\", \"procedure1\"],\n",
    "        \"label\": 1.0,\n",
    "    },\n",
    "    {\n",
    "        \"patient_id\": \"patient-1\",\n",
    "        \"visit_id\": \"visit-1\",\n",
    "        \"visits\": [\"diagnosis3\"],\n",
    "        \"label\": 0.5,\n",
    "    },\n",
    "]\n",
    "\n",
    "ts_dataset = SampleDataset(\n",
    "    samples=ts_samples,\n",
    "    input_schema={\"visits\": \"sequence\"},\n",
    "    output_schema={\"label\": \"regression\"},\n",
    "    dataset_name=\"timeseries_demo\",\n",
    ")\n",
    "\n",
    "# Time-series VAE model\n",
    "ts_model = VAE(\n",
    "    dataset=ts_dataset,\n",
    "    feature_keys=[\"visits\"],\n",
    "    label_key=\"label\",\n",
    "    mode=\"regression\",\n",
    "    input_type=\"timeseries\",  # New parameter\n",
    "    hidden_dim=64,\n",
    ")\n",
    "\n",
    "print(\"Time-series VAE created\")\n",
    "print(f\"Input type: {ts_model.input_type}\")\n",
    "print(f\"Has embedding model: {hasattr(ts_model, 'embedding_model')}\")\n",
    "print(f\"Has RNN encoder: {hasattr(ts_model, 'encoder_rnn')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The enhanced VAE now supports:\n",
    "- **Image generation**: Unconditional and conditional\n",
    "- **Time-series modeling**: For sequential medical data\n",
    "- **Flexible embeddings**: Integrated with PyHealth's EmbeddingModel\n",
    "\n",
    "Key new parameters:\n",
    "- `input_type`: 'image' or 'timeseries'\n",
    "- `conditional_feature_keys`: List of keys for conditional generation\n",
    "\n",
    "This enables more sophisticated generative models for medical data analysis and synthesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
